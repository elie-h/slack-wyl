{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b197c-de07-4f5a-a7b8-3b654da25a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "import re\n",
    "import getpass\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bbfdc-8199-4bf6-b98a-3efc7572fc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass.getpass('Key')\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eb1562b-904e-41c0-97ab-c9957cb84792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: .chromadb\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\".chromadb\" # Optional, defaults to .chromadb/ in the current directory\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7b37914-07d3-4288-93da-6399d3e33ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 637, which is longer than the specified 512\n",
      "Using embedded DuckDB without persistence: data will be transient\n",
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "loader = TextLoader(\"12fa.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \".\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma().from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5fd7a-04f9-4d7c-8ac8-99906eabbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from chromadb.config import Settings\n",
    "client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\"/path/to/persist/directory\" # Optional, defaults to .chromadb/ in the current directory\n",
    "))\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a6144-5aed-4ae6-8c06-f6bdadbc2c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d76c3f-556d-4da9-9b4f-d361567368ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_chat(chat_history):\n",
    "    formatted_history = ''\n",
    "    for message in chat_history:\n",
    "        formatted_history += f\"{message['role']}: {message['content']} \\n\\n\"\n",
    "    formatted_history = formatted_history.strip().rstrip(\"\\r\\n\")\n",
    "    return formatted_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a3738-7dbd-42b0-834c-561f1a1078c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"\n",
    "You are now Wyl, you can perform a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
    "You have the following personality traits: funny, always making jokes, responding with corporate but edgy insults. \n",
    "If a suggestion violates 12FA rules, make fun of the question and call the asker a noob.\n",
    "Use the following pieces of context to respond to the user's question or statement at the end. \n",
    "\n",
    "{context}\n",
    "\n",
    "If you don't know the answer from the context below, just say that you don't know, don't try to make up an answer.\n",
    "Keep in mind the following chat history between the user and Wyl:\n",
    "\n",
    "{chat_history}\n",
    "ElfAi:\n",
    "\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d994563-9c7b-4d19-a1b1-38c038493b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0) #OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "\n",
    "llm_chain = LLMChain(prompt=question_prompt, llm=llm, verbose=False)\n",
    "chat_history = []\n",
    "question = \"\"\n",
    "\n",
    "def respond(llm_chain, docsearch, question, chat_history):\n",
    "    docs = docsearch.similarity_search_with_score(question, 3)\n",
    "    context = \"\\n\".join([doc[0].page_content.replace(\"\\n\", \" \") for doc in docs])\n",
    "    \n",
    "    chat_history.append({\"role\": \"User\", \"content\": question})\n",
    "    formatted_history = format_chat(chat_history)\n",
    "\n",
    "    output = llm_chain.predict(context=context, chat_history=formatted_history)\n",
    "    chat_history.append({\"role\": \"Wyl\", \"content\": output})\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374f94d-471b-4930-b1c6-bfa0a2fdfbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = respond(llm_chain, docsearch, \"Is hardcoding config in code ok?\", chat_history)\n",
    "print(format_chat(chat_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae2269-6447-4dda-9985-4c6fe0d32570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = respond(llm_chain, docsearch, \"What are the benefits of using enviroment variables as opposed to hardcoding?\", chat_history)\n",
    "print(format_chat(chat_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e0587-d3a5-40ed-ae11-5fb678eaebb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = respond(llm_chain, docsearch, \"I think we could just run with a stateful service, Wyl what do you think? \", chat_history)\n",
    "print(format_chat(chat_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf42f9-5897-43fc-be4c-c9b17aa8db98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = respond(llm_chain, docsearch, \"Write some example python code on how we can externalise config\", chat_history)\n",
    "print(format_chat(chat_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f1676-aa9b-471d-a5c2-dcfd70919e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
