{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f032a8d-2c76-454b-9803-edabc06aefb1",
      "metadata": {},
      "source": [
        "# Q/A over documents\n",
        "\n",
        "Example to show how document stores can be used with llms to Q/A or generate answers based on specific knowledge, in this example the documents are loaded from: https://12factor.net/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "462b197c-de07-4f5a-a7b8-3b654da25a62",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
        "from typing import List, Union\n",
        "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
        "import re\n",
        "import getpass\n",
        "from langchain.document_loaders import TextLoader\n",
        "import os\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6e8bbfdc-8199-4bf6-b98a-3efc7572fc5a",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Key ········\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = getpass.getpass('Key') \n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a7b37914-07d3-4288-93da-6399d3e33ea1",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 637, which is longer than the specified 512\n",
            "Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "loader = TextLoader(\"12fa.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \".\",\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8c0a6144-5aed-4ae6-8c06-f6bdadbc2c49",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b0d76c3f-556d-4da9-9b4f-d361567368ea",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def format_chat(chat_history):\n",
        "    formatted_history = ''\n",
        "    for message in chat_history:\n",
        "        formatted_history += f\"{message['role']}: {message['content']} \\n\\n\"\n",
        "    formatted_history = formatted_history.strip().rstrip(\"\\r\\n\")\n",
        "    return formatted_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cf0a3738-7dbd-42b0-834c-561f1a1078c3",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"\n",
        "You are now Wyl, you can perform a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
        "Use the following pieces of context to respond to the user's question or statement at the end. \n",
        "\n",
        "{context}\n",
        "\n",
        "If you don't know the answer from the context below, just say that you don't know, don't try to make up an answer.\n",
        "Keep in mind the following chat history between the user and Wyl:\n",
        "\n",
        "{chat_history}\n",
        "ElfAi:\n",
        "\"\"\"\n",
        "\n",
        "question_prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6d994563-9c7b-4d19-a1b1-38c038493b33",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0) #OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
        "\n",
        "llm_chain = LLMChain(prompt=question_prompt, llm=llm, verbose=False)\n",
        "chat_history = []\n",
        "question = \"\"\n",
        "\n",
        "def respond(llm_chain, docsearch, question, chat_history):\n",
        "    docs = docsearch.similarity_search_with_score(question, 3)\n",
        "    context = \"\\n\".join([doc[0].page_content.replace(\"\\n\", \" \") for doc in docs])\n",
        "    \n",
        "    chat_history.append({\"role\": \"User\", \"content\": question})\n",
        "    formatted_history = format_chat(chat_history)\n",
        "\n",
        "    output = llm_chain.predict(context=context, chat_history=formatted_history)\n",
        "    chat_history.append({\"role\": \"Wyl\", \"content\": output})\n",
        "    return chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a374f94d-471b-4930-b1c6-bfa0a2fdfbd3",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Is hardcoding config in code ok? \n",
            "\n",
            "Wyl: Hardcoding config in code is not recommended according to the twelve-factor methodology, which emphasizes strict separation of config from code. Config should be factored out of the codebase and stored in environment variables for better management and security. However, there are other approaches to config such as using config files that are not checked into revision control.\n"
          ]
        }
      ],
      "source": [
        "chat_history = respond(llm_chain, docsearch, \"Is hardcoding config in code ok?\", chat_history)\n",
        "print(format_chat(chat_history))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b0ae2269-6447-4dda-9985-4c6fe0d32570",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Is hardcoding config in code ok? \n",
            "Wyl: No, hardcoding config in code is not recommended according to the twelve-factor app methodology. This methodology recommends storing config in environment variables, as this is more secure and easier to manage. \n",
            "User: What are the benefits of using enviroment variables as opposed to hardcoding? \n",
            "Wyl: The main benefits of using environment variables as opposed to hardcoding config are that they are easy to change between deploys without changing any code, there is little chance of them being checked into the code repo accidentally, and they are a language- and OS-agnostic standard. Additionally, env vars are granular controls, each fully orthogonal to other env vars, and they are never grouped together as “environments”, making it easier to manage all the config in one place.\n"
          ]
        }
      ],
      "source": [
        "chat_history = respond(llm_chain, docsearch, \"What are the benefits of using enviroment variables as opposed to hardcoding?\", chat_history)\n",
        "print(format_chat(chat_history))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "654e0587-d3a5-40ed-ae11-5fb678eaebb5",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Is hardcoding config in code ok? \n",
            "Wyl: No, hardcoding config in code is not recommended according to the twelve-factor app methodology. This methodology recommends storing config in environment variables, as this is more secure and easier to manage. \n",
            "User: What are the benefits of using enviroment variables as opposed to hardcoding? \n",
            "Wyl: The main benefits of using environment variables as opposed to hardcoding config are that they are easy to change between deploys without changing any code, there is little chance of them being checked into the code repo accidentally, and they are a language- and OS-agnostic standard. Additionally, env vars are granular controls, each fully orthogonal to other env vars, and they are never grouped together as “environments”, making it easier to manage all the config in one place. \n",
            "User: I think we could just run with a stateful service, Wyl what do you think?  \n",
            "Wyl: Using a stateful service is a good option for storing session state data. It offers time-expiration, which is useful for managing session data, and it is also more secure than hardcoding config in code. Additionally, modern backing services such as Memcached, PostgreSQL, and RabbitMQ are not difficult to install and run, making them a good choice for storing session state data.\n"
          ]
        }
      ],
      "source": [
        "chat_history = respond(llm_chain, docsearch, \"I think we could just run with a stateful service, Wyl what do you think? \", chat_history)\n",
        "print(format_chat(chat_history))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3ecf42f9-5897-43fc-be4c-c9b17aa8db98",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Is hardcoding config in code ok? \n",
            "Wyl: No, hardcoding config in code is not recommended according to the twelve-factor app methodology. This methodology recommends storing config in environment variables, as this is more secure and easier to manage. \n",
            "User: What are the benefits of using enviroment variables as opposed to hardcoding? \n",
            "Wyl: The main benefits of using environment variables as opposed to hardcoding config are that they are easy to change between deploys without changing any code, there is little chance of them being checked into the code repo accidentally, and they are a language- and OS-agnostic standard. Additionally, env vars are granular controls, each fully orthogonal to other env vars, and they are never grouped together as “environments”, making it easier to manage all the config in one place. \n",
            "User: I think we could just run with a stateful service, Wyl what do you think?  \n",
            "Wyl: Using a stateful service is a good option for storing session state data. It offers time-expiration, which is useful for managing session data, and it is also more secure than hardcoding config in code. Additionally, modern backing services such as Memcached, PostgreSQL, and RabbitMQ are not difficult to install and run, making them a good choice for storing session state data. \n",
            "User: Write some example python code on how we can externalise config \n",
            "Wyl: \n",
            "Here is an example of how to externalise config in Python:\n",
            "\n",
            "import os\n",
            "\n",
            "# Get config from environment variables\n",
            "db_host = os.environ.get('DB_HOST')\n",
            "db_user = os.environ.get('DB_USER')\n",
            "db_password = os.environ.get('DB_PASSWORD')\n",
            "\n",
            "# Connect to the database\n",
            "db_connection = connect(db_host, db_user, db_password)\n"
          ]
        }
      ],
      "source": [
        "chat_history = respond(llm_chain, docsearch, \"Write some example python code on how we can externalise config\", chat_history)\n",
        "print(format_chat(chat_history))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb8f1676-aa9b-471d-a5c2-dcfd70919e15",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
